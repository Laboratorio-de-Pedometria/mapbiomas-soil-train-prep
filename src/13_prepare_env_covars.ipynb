{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# title: MapBiomas Soil\n",
    "# subtitle: Prepare environmental covariates\n",
    "# author: Alessandro Samuel-Rosa and Taciara Zborowski Horst\n",
    "# data: 2025\n",
    "\n",
    "# Import necessary libraries\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Identify working directory, saving the path to a variable\n",
    "src_dir = os.getcwd()\n",
    "work_dir = os.path.dirname(src_dir)\n",
    "\n",
    "# Read the TXT file 'data/12_soildata.txt' with the soil data from the 'data' folder processed\n",
    "# in the previous script '12_prepare_soil_covars.R'\n",
    "# Field separator: tab\n",
    "file_path = os.path.join(work_dir, 'data', '12_soildata.txt')\n",
    "soildata_df = pd.read_csv(file_path, sep='\\t', low_memory=False)\n",
    "print(soildata_df.shape)\n",
    "# (54555, 66)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the TXT file 'data/13_soildata.txt' with the coordinates from the 'data' folder\n",
    "# Field separator: tab. Decimal separator: comma\n",
    "# file_path = os.path.join(work_dir, 'data', '13_soildata.txt')\n",
    "# df2 = pd.read_csv(file_path, sep='\\t', low_memory=False)\n",
    "# print(df2.shape)\n",
    "\n",
    "# Compare the two data frames to check if there were changes in the data\n",
    "# Use equals() method to compare the two data frames\n",
    "# The result is a boolean value, True if the data frames are equal, False otherwise\n",
    "# We consider only the following variables in the comparison:\n",
    "# dataset_id, observacao_id, coord_x, coord_y, data_ano\n",
    "# df1 = soildata_df[['dataset_id', 'observacao_id', 'coord_x', 'coord_y', 'data_ano']]\n",
    "# df2 = df2[['dataset_id', 'observacao_id', 'coord_x', 'coord_y', 'data_ano']]\n",
    "# if df1.equals(df2):\n",
    "#     print(\"The DataFrames are identical.\")\n",
    "# else:\n",
    "#     print(\"The DataFrames are different.\")\n",
    "# del df1, df2\n",
    "# 29683 layers (GET FROM PREVIOUS SCRIPT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNIQUE EVENTS WITH GEOGRAPHIC COORDINATES\n",
    "target = 16568 # GET VALUE FROM PREVIOUS SCRIPT!\n",
    "\n",
    "# Filter out soil layers without geographic coordinates\n",
    "# coord_x and coord_y are the columns with the geographic coordinates\n",
    "soildata_xy = soildata_df[soildata_df['coord_x'].notnull()]\n",
    "soildata_xy = soildata_xy[soildata_xy['coord_y'].notnull()]\n",
    "\n",
    "# Remove all duplicates based on the following columns:\n",
    "# \"dataset_id\", \"observacao_id\", \"coord_x\", \"coord_y\", \"data_ano\"\n",
    "# The first occurrence is kept, and the others are removed\n",
    "soildata_xy = soildata_xy[['dataset_id', 'observacao_id', 'coord_x', 'coord_y', 'data_ano']]\n",
    "soildata_xy = soildata_xy.drop_duplicates()\n",
    "print(\n",
    "  'There should be', target, 'events:', target == soildata_xy.shape[0],\n",
    "  '\\nThere are', soildata_xy.shape[0], 'events')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the first and last 5 rows of the data frame\n",
    "print(soildata_xy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import ee\n",
    "import geemap\n",
    "import numpy as np  # Import numpy to access np.nan\n",
    "\n",
    "# Initialize the Earth Engine API\n",
    "# ee.Authenticate()\n",
    "ee.Initialize(project='mapbiomas-solos-workspace')\n",
    "\n",
    "# 1. Clean the DataFrame: Replace all NaN values with None\n",
    "soildata_xy_cleaned = soildata_xy.replace({np.nan: None})\n",
    "\n",
    "# 2. Convert the *cleaned* DataFrame to an Earth Engine Feature Collection\n",
    "soildata_fc = geemap.df_to_ee(soildata_xy_cleaned, latitude='coord_y', longitude='coord_x')\n",
    "print(soildata_fc.size().getInfo(), 'features')\n",
    "\n",
    "# 3. Function to split a feature collection (sampling points) into chunks\n",
    "def split_sampling_points(fc, chunk_size):\n",
    "    features = fc.toList(fc.size())\n",
    "    chunks = [features.slice(i, i + chunk_size) for i in range(0, features.size().getInfo(), chunk_size)]\n",
    "    return [ee.FeatureCollection(chunk) for chunk in chunks]\n",
    "# Split the sample points into subsets of 1000 points each\n",
    "# This is necessary to avoid timeout errors\n",
    "chunk_size = 1000\n",
    "soildata_chunks = split_sampling_points(soildata_fc, chunk_size)\n",
    "print(len(soildata_chunks), 'chunks')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SoilGrids 250m (Hengl et al., 2017)\n",
    "# WRB Soil Reference Groups at 250-m spatial resolution\n",
    "wrb_path = \"projects/mapbiomas-workspace/SOLOS/COVARIAVEIS/WRB_ALL_SOILS_SOILGRIDS_30M_GAPFILL\"\n",
    "wrb_bands = ['Ferralsols', 'Histosols', 'Nitisols', 'Vertisols', 'Plinthosols', 'Arenosols',\n",
    "             'Podzols', 'Chernozems', 'Phaeozems', 'Umbrisols', 'Leptosols', 'Regosols',\n",
    "             'Gleysols', 'Planosols', 'Stagnosols', 'Alisols', 'Luvisols', 'Acrisols', 'Lixisols']\n",
    "wrb_image = ee.Image(wrb_path).select(wrb_bands)\n",
    "print(wrb_image.getInfo())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Black Soils (FAO, 2022)\n",
    "# Probability of occurrence of black soils at 1-km spatial resolution\n",
    "black_path = \"projects/mapbiomas-workspace/SOLOS/COVARIAVEIS/FAO_2022_BLACKSOIL_1KM\"\n",
    "black_image = ee.Image(black_path)\n",
    "print(black_image.getInfo())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SoilGrids 250m (Poggio et al., 2021)\n",
    "# Soil properties at 250-m spatial resolution at six standard depths (0-5cm, 5-15cm, 15-30cm,\n",
    "# 30-60cm, 60-100cm, 100-200cm)\n",
    "\n",
    "# Soil bulk density (bdod_mean)\n",
    "bdod_path = \"projects/soilgrids-isric/bdod_mean\"\n",
    "bdod_bands = ['bdod_0-5cm_mean', 'bdod_5-15cm_mean', 'bdod_15-30cm_mean',\n",
    "              'bdod_30-60cm_mean', 'bdod_60-100cm_mean', 'bdod_100-200cm_mean']\n",
    "bdod_image = ee.Image(bdod_path).select(bdod_bands)\n",
    "\n",
    "# Clay content (clay_mean)\n",
    "clay_path = \"projects/soilgrids-isric/clay_mean\"\n",
    "clay_bands = ['clay_0-5cm_mean', 'clay_5-15cm_mean', 'clay_15-30cm_mean',\n",
    "              'clay_30-60cm_mean', 'clay_60-100cm_mean', 'clay_100-200cm_mean']\n",
    "clay_image = ee.Image(clay_path).select(clay_bands)\n",
    "\n",
    "# Sand content (sand_mean)\n",
    "sand_path = \"projects/soilgrids-isric/sand_mean\"\n",
    "sand_bands = ['sand_0-5cm_mean', 'sand_5-15cm_mean', 'sand_15-30cm_mean', \n",
    "              'sand_30-60cm_mean', 'sand_60-100cm_mean', 'sand_100-200cm_mean']\n",
    "sand_image = ee.Image(sand_path).select(sand_bands)\n",
    "\n",
    "# Soil organic carbon (soc_mean)\n",
    "soc_path = \"projects/soilgrids-isric/soc_mean\"\n",
    "soc_bands = ['soc_0-5cm_mean', 'soc_5-15cm_mean', 'soc_15-30cm_mean',\n",
    "             'soc_30-60cm_mean', 'soc_60-100cm_mean', 'soc_100-200cm_mean']\n",
    "soc_image = ee.Image(soc_path).select(soc_bands)\n",
    "\n",
    "# Volume of coarse fragments (cfvo_mean)\n",
    "cfvo_path = \"projects/soilgrids-isric/cfvo_mean\"\n",
    "cfvo_bands = ['cfvo_0-5cm_mean', 'cfvo_5-15cm_mean', 'cfvo_15-30cm_mean',\n",
    "              'cfvo_30-60cm_mean', 'cfvo_60-100cm_mean', 'cfvo_100-200cm_mean']\n",
    "cfvo_image = ee.Image(cfvo_path).select(cfvo_bands)\n",
    "\n",
    "# Stack all soil property images into a single image\n",
    "soil_image = (bdod_image\n",
    "              .addBands(clay_image)\n",
    "              .addBands(sand_image)\n",
    "              .addBands(soc_image)\n",
    "              .addBands(cfvo_image))\n",
    "print(soil_image.getInfo())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Geomorphometry\n",
    "\n",
    "# Digital Elevation Model (DEM)\n",
    "dem_path = \"MERIT/DEM/v1_0_3\"\n",
    "dem_image = ee.Image(dem_path).select('dem').round()\n",
    "\n",
    "# Land surface variables (Amatulli et al., 2020, 2018)\n",
    "land_path = \"projects/mapbiomas-workspace/SOLOS/COVARIAVEIS/OT_GEOMORPHOMETRY_90m\"\n",
    "\n",
    "# Altitude\n",
    "altitude_image = ee.Image(land_path).select('elevation').round()\n",
    "\n",
    "# Slope\n",
    "slope_image = ee.Image(land_path).select('slope').multiply(3.141593).divide(180).tan().multiply(100).round()\n",
    "\n",
    "# Convergence\n",
    "convergence_image = ee.Image(land_path).select('convergence').round()\n",
    "\n",
    "# Compound Topographic Index (CTI)\n",
    "# cti * 10\n",
    "cti_image = ee.Image(land_path).select('cti').multiply(10).round()\n",
    "\n",
    "# Eastness\n",
    "# eastness * 100\n",
    "eastness_image = ee.Image(land_path).select('eastness').multiply(100).round()\n",
    "\n",
    "# Northness\n",
    "# northness * 100\n",
    "northness_image = ee.Image(land_path).select('northness').multiply(100).round()\n",
    "\n",
    "# Profile Curvature\n",
    "# pcurv * 1000\n",
    "pcurv_image = ee.Image(land_path).select('pcurv').multiply(1000).round()\n",
    "\n",
    "# Roughness\n",
    "roughness_image = ee.Image(land_path).select('roughness').round()\n",
    "\n",
    "# Stream Power Index (SPI)\n",
    "spi_image = ee.Image(land_path).select('spi').add(1).log10().multiply(100).round()\n",
    "\n",
    "# Elevation Standard Deviation\n",
    "elev_stdev_image = ee.Image(land_path).select('elev_stdev').round()\n",
    "\n",
    "# Maximum Multiscale Deviation\n",
    "dev_magnitude_image = ee.Image(land_path).select('dev_magnitude').round()\n",
    "\n",
    "# Geomorphon (geomorphological forms)\n",
    "# geom\n",
    "geom_image = ee.Image(land_path).select('geom').round()\n",
    "\n",
    "# Stack all land surface variable images into a single image\n",
    "dem_image = (dem_image\n",
    "             .addBands(slope_image)\n",
    "             .addBands(convergence_image)\n",
    "             .addBands(cti_image)\n",
    "             .addBands(eastness_image)\n",
    "             .addBands(northness_image)\n",
    "             .addBands(pcurv_image)\n",
    "             .addBands(roughness_image)\n",
    "             .addBands(spi_image)\n",
    "             .addBands(elev_stdev_image)\n",
    "             .addBands(dev_magnitude_image)\n",
    "             .addBands(geom_image))\n",
    "print(dem_image.getInfo())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Structural Provinces (IBGE, 2019c)\n",
    "\n",
    "# First level: Provinces\n",
    "province_path = \"projects/mapbiomas-workspace/SOLOS/COVARIAVEIS/IBGE_PROVINCIAIS_ESTRUTURAIS_250mil_2025/IBGE_PROVINCIAS_250MIL_DUMMY\"\n",
    "province_image = ee.Image(province_path)\n",
    "\n",
    "# Second level: Subprovinces\n",
    "subprovince_path = \"projects/mapbiomas-workspace/SOLOS/COVARIAVEIS/IBGE_PROVINCIAIS_ESTRUTURAIS_250mil_2025/subprovincias\"\n",
    "subprovince_image = ee.Image(subprovince_path)\n",
    "\n",
    "# Stack all structural province images into a single image\n",
    "geo_image = (province_image\n",
    "             .addBands(subprovince_image))\n",
    "print(geo_image.getInfo())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distance to specific landforms (MapBiomas, 2024)\n",
    "\n",
    "# Rock outcrops\n",
    "rock_path = 'projects/mapbiomas-workspace/SOLOS/COVARIAVEIS/DISTANCE_C10_v1/distance_afloramento_rochoso_c10_v1'\n",
    "rock_image = ee.Image(rock_path).rename('Distance_to_rock')\n",
    "\n",
    "# Sand deposits (beaches, dunes, sandy areas)\n",
    "sand_path = 'projects/mapbiomas-workspace/SOLOS/COVARIAVEIS/DISTANCE_C10_v1/distance_praia_duna_areal_c10_v1'\n",
    "sand_image = ee.Image(sand_path)\n",
    "\n",
    "# Stack all distance to landform images into a single image\n",
    "dist_image = rock_image.addBands(sand_image)\n",
    "print(dist_image.getInfo())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract covariate values to sampling points\n",
    "# This takes about 15 minutes to run...\n",
    "\n",
    "# Stack the images into a single multiband image\n",
    "static_image = (wrb_image # Soil Reference Groups\n",
    "                .addBands([\n",
    "                    black_image, # Black Soils (FAO, 2022)\n",
    "                    soil_image, # SoilGrids\n",
    "                    dem_image, # Land Surface Variables\n",
    "                    geo_image, # Structural Provinces\n",
    "                    dist_image # Distance to specific landforms\n",
    "                ]))\n",
    "                   \n",
    "# Initialize an empty list to store DataFrames\n",
    "dataframes = []\n",
    "\n",
    "# Loop over each subset and sample the data\n",
    "for chunk in soildata_chunks:\n",
    "    sampled_points = geemap.extract_values_to_points(chunk, static_image, scale=250)\n",
    "    sampled_df = geemap.ee_to_df(sampled_points)\n",
    "    dataframes.append(sampled_df)\n",
    "\n",
    "# Concatenate all DataFrames into a single DataFrame\n",
    "static_df = pd.concat(dataframes, ignore_index=True)\n",
    "\n",
    "# Rename columns\n",
    "# Replace dash with underscores\n",
    "static_df.columns = static_df.columns.str.replace('-', '_')\n",
    "# Remove _mean from column names\n",
    "static_df.columns = static_df.columns.str.replace('_mean', '')\n",
    "\n",
    "# Print column names\n",
    "print(static_df.columns)\n",
    "\n",
    "# Check if number of rows of static_df is the same as that of soildata_xy\n",
    "# If the number of rows is the same, the merge was successful\n",
    "target = soildata_xy.shape[0]\n",
    "print(\n",
    "  '\\nThere should be', target, 'events:',\n",
    "  target == static_df.shape[0], '\\nThere are', static_df.shape[0], 'events')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute summary statistics of the static covariates\n",
    "summary = static_df.describe()\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "# MapBiomas LULC Collection\n",
    "# (this takes about 10 minutes to run)\n",
    "\n",
    "# Import the MapBiomas Collection 9.0\n",
    "collection = 'projects/mapbiomas-public/assets/brazil/lulc/collection9/mapbiomas_collection90_integration_v1'\n",
    "mapbiomas_image = ee.Image(collection)\n",
    "\n",
    "# Initialize an empty list to store DataFrames\n",
    "dataframes = []\n",
    "\n",
    "# Loop over each subset and sample the data\n",
    "for chunk in soildata_chunks:\n",
    "    sampled_points = geemap.extract_values_to_points(chunk, mapbiomas_image, scale=30)\n",
    "    sampled_df = geemap.ee_to_df(sampled_points)\n",
    "    dataframes.append(sampled_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate all DataFrames into a single DataFrame\n",
    "mapbiomas_df = pd.concat(dataframes, ignore_index=True)\n",
    "\n",
    "# Rename columns\n",
    "# Remove classification_ prefix from column names\n",
    "mapbiomas_df.columns = mapbiomas_df.columns.str.replace('classification_', '')\n",
    "\n",
    "# Get LULC class at the year of sampling (data_ano)\n",
    "# Each column in the MapBiomas dataset represents a year. The column name is the year of the\n",
    "# classification. The value is the class code. We need to extract the class code for the year of\n",
    "# sampling, which is stored in the data_ano column.\n",
    "# Step 1: Create a new column YEAR based on data_ano\n",
    "mapbiomas_df['YEAR'] = mapbiomas_df['data_ano']\n",
    "# Step 2: If YEAR is less than 1985, set it to 0 (no data)\n",
    "mapbiomas_df.loc[mapbiomas_df['YEAR'] < 1985, 'YEAR'] = 0\n",
    "# Step 3: Find the column index for each YEAR\n",
    "lulc_idx = mapbiomas_df.columns.get_indexer(mapbiomas_df['YEAR'].astype(str))\n",
    "# Step 4: Extract the class code for each row based on the data_ano column\n",
    "lulc = mapbiomas_df.to_numpy()\n",
    "lulc = lulc[range(len(lulc)), lulc_idx]\n",
    "# Step 5: Convert the extracted class codes to strings and assign them to a new column lulc\n",
    "mapbiomas_df['lulc'] = lulc.astype(str)\n",
    "# Step 6: Drop the YEAR column\n",
    "mapbiomas_df = mapbiomas_df.drop(columns = ['YEAR'])\n",
    "\n",
    "# Some columns of mapbiomas_df are named with the year of the classification, ranging from 1985 to\n",
    "# the present year. This columns need to be dropped.\n",
    "# Drop columns with years as column names\n",
    "current_year = datetime.now().year\n",
    "years_to_drop = [str(year) for year in range(1985, current_year + 1)]\n",
    "mapbiomas_df.drop(columns=years_to_drop, inplace=True, errors='ignore')\n",
    "\n",
    "# Reclassify the land cover classes\n",
    "forest_codes = ['1.0', '3.0', '4.0', '5.0', '6.0', '49.0']\n",
    "nonforest_codes = ['10.0', '11.0', '12.0', '32.0', '29.0', '50.0']\n",
    "pasture_codes = ['15.0']\n",
    "agriculture_codes = [\n",
    "    '14.0', '18.0', '19.0', '39.0', '20.0', '40.0', '62.0', '41.0', '36.0', '46.0', '47.0', '48.0',\n",
    "    '21.0', '35.0']\n",
    "forestry_codes = ['9.0']\n",
    "nonvegetation_codes = ['22.0', '23.0', '24.0', '30.0', '25.0', '26.0', '33.0', '31.0', '27.0']\n",
    "na_codes = ['0', '0.0', 'nan']\n",
    "mapbiomas_df['lulc'] = mapbiomas_df['lulc'].replace(forest_codes, 'forest')\n",
    "mapbiomas_df['lulc'] = mapbiomas_df['lulc'].replace(nonforest_codes, 'nonforest')\n",
    "mapbiomas_df['lulc'] = mapbiomas_df['lulc'].replace(pasture_codes, 'pasture')\n",
    "mapbiomas_df['lulc'] = mapbiomas_df['lulc'].replace(agriculture_codes, 'agriculture')\n",
    "mapbiomas_df['lulc'] = mapbiomas_df['lulc'].replace(forestry_codes, 'forestry')\n",
    "mapbiomas_df['lulc'] = mapbiomas_df['lulc'].replace(nonvegetation_codes, 'nonvegetation')\n",
    "mapbiomas_df['lulc'] = mapbiomas_df['lulc'].replace(na_codes, pd.NA)\n",
    "\n",
    "# Print summary of the land cover classes, including NA\n",
    "print('\\nDistribution of land cover/land use classes (including NA):')\n",
    "print(mapbiomas_df['lulc'].value_counts(dropna=False))\n",
    "\n",
    "# Check if number of rows of mapbiomas_df is the same as that of soildata_xy\n",
    "# If the number of rows is the same, the sampling was successful\n",
    "target = soildata_xy.shape[0]\n",
    "print(\n",
    "  '\\nThere should be', target, 'events:',\n",
    "  target == mapbiomas_df.shape[0], '\\nThere are', mapbiomas_df.shape[0], 'events')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Geomorphometry 90 m\n",
    "\n",
    "# Sample the terrain attributes\n",
    "# projects/mapbiomas-workspace/SOLOS/COVARIAVEIS/OT_GEOMORPHOMETRY_90m\n",
    "# (this takes about 10 minutes to run)\n",
    "dem = ee.Image(\"projects/mapbiomas-workspace/SOLOS/COVARIAVEIS/OT_GEOMORPHOMETRY_90m\")\n",
    "\n",
    "# Initialize an empty list to store DataFrames\n",
    "dem_dfs = []\n",
    "\n",
    "# Loop over each subset and sample the data\n",
    "for chunk in soildata_chunks:\n",
    "    sampled_points = geemap.extract_values_to_points(chunk, dem, scale=90)\n",
    "    sampled_df = geemap.ee_to_df(sampled_points)\n",
    "    dem_dfs.append(sampled_df)\n",
    "\n",
    "# Concatenate all DataFrames into a single DataFrame\n",
    "dem_dataframe = pd.concat(dem_dfs, ignore_index=True)\n",
    "\n",
    "# print column names\n",
    "print(dem_dataframe.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute summary statistics of the Geomorphometry data\n",
    "summary = dem_dataframe.describe()\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge data sampled from SoilGrids, MapBiomas, and Geomorphometry into soildata_df\n",
    "# Keep all rows from soildata_df\n",
    "merge_columns = ['dataset_id', 'observacao_id', 'coord_x', 'coord_y', 'data_ano']\n",
    "output_df = soildata_df.merge(soilgrids_df, on = merge_columns, how = 'left')\n",
    "output_df = output_df.merge(mapbiomas_df, on = merge_columns, how = 'left')\n",
    "output_df = output_df.merge(dem_dataframe, on = merge_columns, how = 'left')\n",
    "\n",
    "# # Fill empty cells in the 'lulc' column with 'unknown'\n",
    "# output_df['lulc'] = output_df['lulc'].fillna('unknown')\n",
    "\n",
    "# Check if number of rows of output_df is the same as that of soildata_df\n",
    "# If the number of rows is the same, the merge was successful\n",
    "target = soildata_df.shape[0]\n",
    "print(\n",
    "  '\\nThere should be', target, 'layers:',\n",
    "  target == output_df.shape[0], '\\nThere are', output_df.shape[0], 'layers')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the output_df to a TXT file\n",
    "# Field separator: tab. Decimal separator: comma\n",
    "file_path = os.path.join(work_dir, 'data', '21_soildata_soc.txt')\n",
    "output_df.to_csv(file_path, sep='\\t', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
